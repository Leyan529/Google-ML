{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84x4Fxc5lzFv"
   },
   "source": [
    "# Introduction to Fairness in ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8daw3YOIAXH"
   },
   "source": [
    "## Disclaimer\n",
    "This exercise explores just a small subset of  ideas and techniques relevant to fairness in machine learning; it is not the whole story!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFxZOg55lWJE"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Increase awareness of different types of biases that can manifest in model data.\n",
    "* Explore feature data to proactively identify potential sources of bias before training a model.\n",
    "* Evaluate model performace by subgroup rather than in aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-K-xqksm-X3"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this exercise, you'll explore datasets and evaluate classifiers with *fairness* in mind, noting the ways undesirable biases can creep into machine learning (ML).\n",
    "\n",
    "Throughout, you will see **FairAware** tasks, which provide opportunities to contextualize ML processes with respect to fairness. In performing these tasks, you'll identify biases and consider the long-term impact of model predictions if these biases are not addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXkkHYyJ98_k"
   },
   "source": [
    "## About the Dataset and Prediction Task\n",
    "\n",
    "In this exercise, you'll work with the [Adult Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income), which is commonly used in machine learning literature. This data was extracted from the [1994 Census bureau database](http://www.census.gov/en.html) by Ronny Kohavi and Barry Becker.\n",
    "\n",
    "Each example in the dataset contains the following demographic data for a set of individuals who took part in the 1994 Census:\n",
    "\n",
    "### Numeric Features\n",
    "*   `age`: The age of the individual in years.\n",
    "*   `fnlwgt`: The number of individuals the Census Organizations believes that set of observations represents.\n",
    "*   `education_num`:  An enumeration of the categorical representation of education. The higher the number, the higher the education that individual achieved. For example, an `education_num` of `11` represents `Assoc_voc` (associate degree at a vocational school), an `education_num` of `13` represents `Bachelors`, and an `education_num` of `9` represents `HS-grad` (high school graduate).\n",
    "*   `capital_gain`: Capital gain made by the individual, represented in US Dollars.\n",
    "*   `capital_loss`: Capital loss mabe by the individual, represented in US Dollars.\n",
    "*   `hours_per_week`: Hours worked per week.\n",
    "\n",
    "### Categorical Features\n",
    "*   `workclass`: The individual's type of employer. Examples include: `Private`, `Self-emp-not-inc`, `Self-emp-inc`, `Federal-gov`, `Local-gov`, `State-gov`, `Without-pay`, and `Never-worked`.\n",
    "*   `education`: The highest level of education achieved for that individual.\n",
    "*   `marital_status`: Marital status of the individual. Examples include: `Married-civ-spouse`, `Divorced`, `Never-married`, `Separated`, `Widowed`, `Married-spouse-absent`, and `Married-AF-spouse`.\n",
    "*   `occupation`: The occupation of the individual. Example include: `tech-support`, `Craft-repair`, `Other-service`, `Sales`, `Exec-managerial` and more.\n",
    "*   `relationship`:  The relationship of each individual in a household. Examples include: `Wife`, `Own-child`, `Husband`, `Not-in-family`, `Other-relative`, and `Unmarried`.\n",
    "*   `gender`:  Gender of the individual available only in binary choices: `Female` or `Male`.\n",
    "*   `race`: `White`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo`, `Black`, and `Other`. \n",
    "*   `native_country`: Country of origin of the individual. Examples include: `United-States`, `Cambodia`, `England`, `Puerto-Rico`, `Canada`, `Germany`, `Outlying-US(Guam-USVI-etc)`, `India`, `Japan`, and more.\n",
    "\n",
    "### Prediction Task\n",
    "The prediction task is to **determine whether a person makes over $50,000 US Dollar a year.**\n",
    "\n",
    "### Label\n",
    "*   `income_bracket`: Whether the person makes more than $50,000 US Dollars annually.\n",
    "\n",
    "### Notes on Data Collection\n",
    "\n",
    "All the examples extracted for this dataset meet the following conditions: \n",
    "*   `age` is 16 years or older.\n",
    "*   The adjusted gross income (used to calculate `income_bracket`) is greater than $100 USD annually.\n",
    "*   `fnlwgt` is greater than 0.\n",
    "*   `hours_per_week` is greater than 0.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0RMIktKy8xX"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we should ensure that this Colaboratory notebook will run on TensorFlow 2.x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUsgiVsUeKRR"
   },
   "source": [
    "Next, we'll import the necessary modules to run the code in the rest of this Colaboratory notebook. \n",
    "\n",
    "In addition to importing the usual libraries, this setup code cell also installs [Facets](https://pair-code.github.io/facets/), an open-source tool created by [PAIR](https://research.google/teams/brain/pair/) that contains two robust visualizations we'll be using to aid in understanding and analyzing ML datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "2e_0DJJ8zE29",
    "outputId": "eb15a970-1c36-4e59-beb8-147a889f7fa5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pytorch_lightning as pl\n",
    "# import pytorch_lightning.metrics.sklearns as plm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torchmetrics import AUROC, Accuracy, Precision, Recall\n",
    "    \n",
    "# from matplotlib import pyplot as plt\n",
    "# from matplotlib import rcParams\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For facets\n",
    "import base64\n",
    "\n",
    "from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator\n",
    "from IPython.core.display import HTML, display\n",
    "from barbar import Bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xgIRapb5LaQ"
   },
   "source": [
    "### Load the Adult Dataset\n",
    "\n",
    "With the modules now imported, we can load the Adult dataset into a pandas DataFrame data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TeCNVvVUVS0P",
    "outputId": "6d05c383-4aa1-4533-c766-413878103642"
   },
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    \"https://download.mlcc.google.com/mledu-datasets/adult_census_train.csv\",\n",
    "    names=COLUMNS,\n",
    "    sep=r\"\\s*,\\s*\",\n",
    "    engine=\"python\",\n",
    "    na_values=\"?\",\n",
    ")\n",
    "test_df = pd.read_csv(\n",
    "    \"https://download.mlcc.google.com/mledu-datasets/adult_census_test.csv\",\n",
    "    names=COLUMNS,\n",
    "    sep=r\"\\s*,\\s*\",\n",
    "    skiprows=[0],\n",
    "    engine=\"python\",\n",
    "    na_values=\"?\",\n",
    ")\n",
    "test_df[\"income_bracket\"] = test_df[\"income_bracket\"].str.replace(\"\\.$\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "coilRN-hooja"
   },
   "source": [
    "## Analyzing the Adult Dataset with Facets\n",
    "\n",
    "As mentioned in MLCC, it is important to understand your dataset *before* diving straight into the prediction task. \n",
    "\n",
    "Some important questions to investigate when auditing a dataset for fairness:\n",
    "\n",
    "* **Are there missing feature values for a large number of observations?**\n",
    "* **Are there features that are missing that might affect other features?**\n",
    "* **Are there any unexpected feature values?**\n",
    "* **What signs of data skew do you see?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yCIuAqWA1Pm"
   },
   "source": [
    "To start, we can use [Facets Overview](https://pair-code.github.io/facets/), an interactive visualization tool that can help us explore the dataset. With Facets Overview, we can quickly analyze the distribution of values across the Adult dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MW-qryqs1gig",
    "outputId": "ad7095b9-c40a-40d5-86aa-8a0155aa459c"
   },
   "source": [
    "```python\n",
    "# @title Visualize the Data in Facets\n",
    "fsg = FeatureStatisticsGenerator()\n",
    "dataframes = [{\"table\": train_df, \"name\": \"trainData\"}]\n",
    "censusProto = fsg.ProtoFromDataFrames(dataframes)\n",
    "protostr = base64.b64encode(censusProto.SerializeToString()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
    "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
    "        <facets-overview id=\"elem\"></facets-overview>\n",
    "        <script>\n",
    "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(protostr=protostr)\n",
    "display(HTML(html))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91wjnZFpPWw-"
   },
   "source": [
    "## FairAware Task #1\n",
    "\n",
    "Review the descriptive statistics and histograms for each numerical and continuous feature. Click the **Show Raw Data** button above the histograms for categorical features to see the distribution of values per category.\n",
    "\n",
    "Then, try to answer the following questions from earlier:\n",
    "\n",
    "1. Are there missing feature values for a large number of observations?\n",
    "2. Are there features that are missing that might affect other features?\n",
    "3. Are there any unexpected feature values?\n",
    "4. What signs of data skew do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlF-lQ8yQ69b"
   },
   "source": [
    "### Solution\n",
    "\n",
    "Click below for some insights we uncovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xX_qjj5AQ_Hb"
   },
   "source": [
    "We can see from reviewing the **missing** column that the following categorical features contain missing values:\n",
    "\n",
    "*   workclass\n",
    "*   occupation\n",
    "\n",
    "Now, because it's only a small percentage of samples that contain either a missing workclass value or occupation value, we can safely drop those rows from the data set. If that percentage was much higher, then we would have to consider using a different data set that is more complete. \n",
    "\n",
    "Luckily, in Pandas, there is a convenient way to drop any row containing a missing value in the data set:\n",
    "\n",
    "```\n",
    "# pandas.DataFrame.dropna(how=\"any\", axis=0, inplace=True)\n",
    "```\n",
    "We will use this method prior to training the model when we convert a Pandas DataFrame to a Numpy array.\n",
    "\n",
    "As for the remaining data that does not contain any missing values: if we look at the min/max values and histograms for each numeric feature, then we can pinpoint any extreme outliers in our data set. \n",
    "\n",
    "For `hours_per_week`, we can see that the minimum is 1, which might be a bit surprising, given that most jobs typically require multiple hours of work per week. For `capital_gain` and `capital_loss`, we can see that over 90% of values are 0. Given that capital gains/losses are only registered by individuals who make investments, it's certainly plausible that less than 10% of examples would have nonzero values for these feature, but we may want to take a closer look to verify the values for these features are valid.\n",
    "\n",
    "In looking at the histogram for gender, we see that over two-thirds (approximately 67%) of examples represent males. This strongly suggests data skew, as we would expect the breakdown between genders to be closer to 50/50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKj2hz-Sql7V"
   },
   "source": [
    "### A Deeper Dive\n",
    "\n",
    "To futher explore the dataset, we can use [Facets Dive](https://pair-code.github.io/facets/), a tool that provides an interactive interface where each individual item in the visualization represents a data point. But to use Facets Dive, we need to convert the data to a JSON array.\n",
    "Thankfully the DataFrame method `to_json()` takes care of this for us.\n",
    "\n",
    "Run the cell below to perform the data transform to JSON and also load Facets Dive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "id": "stlklrG_xssF",
    "outputId": "bcdc8605-5178-4ab8-c05a-e519677f7a2e"
   },
   "source": [
    "```python\n",
    "# @title Set the Number of Data Points to Visualize in Facets Dive\n",
    "\n",
    "SAMPLE_SIZE = 5000  # @param\n",
    "\n",
    "train_dive = train_df.sample(SAMPLE_SIZE).to_json(orient=\"records\")\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
    "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
    "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
    "        <script>\n",
    "          var data = {jsonstr};\n",
    "          document.querySelector(\"#elem\").data = data;\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(jsonstr=train_dive)\n",
    "display(HTML(html))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LxqAPDcRDFB2"
   },
   "source": [
    "## FairAware Task #2\n",
    "\n",
    "Use the menus on the left panel of the visualization to change how the data is organized:\n",
    "\n",
    "1. In the **Binning | X-Axis** menu, select **education**, and in the **Color By** and  **Label By** menus, select **income_bracket**. How would you describe the relationship between education level and income bracket?\n",
    "\n",
    "2. Next, in the **Binning | X-Axis** menu, select  **marital_status**, and in the **Color By** and  **Label By** menus, select **gender**. What noteworthy observations can you make about the gender distributions for each marital-status category?\n",
    "\n",
    "As you perform the above tasks, keep the following fairness-related questions in mind:\n",
    "\n",
    "* **What's missing?**\n",
    "* **What's being overgeneralized?**\n",
    "* **What's being underrepresented?**\n",
    "* **How do the variables, and their values, reflect the real world?**\n",
    "* **What might we be leaving out?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZ-9vJgSEpHj"
   },
   "source": [
    "### Solution\n",
    "\n",
    "Click below for some insights we uncovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYpbgdATEx8L"
   },
   "source": [
    "1. In the data set, higher education levels generally tend to correlate with a higher income bracket. An income level of greater than $50,000 is more heavily represented in examples where education level is Bachelor's degree or higher.\n",
    "\n",
    "2. In most marital-status categories, the distribution of male vs. female values is close to 1:1. The one notable exception is \"married-civ-spouse\", where male outnumbers female by more than 5:1. Given that we already discovered in Task #1 that there is a disproportionately high representation of men in the data set, we can now infer that it's married women specifically that are underrepresented in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YVH8hYfSjer"
   },
   "source": [
    "### Summary\n",
    "\n",
    "Plotting histograms, ranking most-to-least common examples, identifying duplicate or missing examples, making sure the training and test sets are similar, computing feature quantiles—**these are all critical analyses to perform on your data.** \n",
    "\n",
    "**The better you know what's going on in your data, the more insight you'll have as to where unfairness might creep in!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ivWw9Wpj67m"
   },
   "source": [
    "## FairAware Task #3\n",
    "\n",
    "Now that you've explored the dataset using Facets, see if you can identify some of the problems that may arise with regard to fairness based on what you've learned about its features.\n",
    "\n",
    "Which of the following features might pose a problem with regard to fairness?\n",
    "\n",
    "### hours_per_week\n",
    "\n",
    "It does seem a little strange to see 'hours_per_week' max out at 99 hours,\n",
    "which could lead to data misrepresentation. One way to address this is by\n",
    "representing 'hours_per_week' as a binary \"working 40 hours/not working 40\n",
    "hours\" feature. Also keep in mind that data was extracted based on work hours\n",
    "being greater than 0. In other words, this feature representation exclude a\n",
    "subpopulation of the US that is not working. This could skew the outcomes of the\n",
    "model.\n",
    "\n",
    "### fnlwgt\n",
    "\n",
    "'fnlwgt' represents the weight of the observations. After fitting the model\n",
    "to this data set, if certain group of individuals end up performing poorly \n",
    "compared to other groups, then we could explore ways of reweighting each data \n",
    "point using this feature.\n",
    "\n",
    "### gender\n",
    "\n",
    "Looking at the ratio between men and women shows how disproportionate the data\n",
    "is compared to the real world where the ratio (at least in the US) is closer to\n",
    "1:1. This could pose a huge probem in performance across gender. Considerable\n",
    "measures may need to be taken to upsample the underrepresented group (in this\n",
    "case, women).\n",
    "\n",
    "### capital_gain / capital_loss\n",
    "\n",
    "As alluded to in Task #1, both 'capital_gain' and 'capital_loss' could be \n",
    "indicative of income status as only individuals who make investments register \n",
    "their capital gains and losses. The caveat is that over 90% of the values in \n",
    "both 'capital_gain' and 'capital_loss' are 0, and it's not entirely clear from \n",
    "the description of the data set why that is the case. That is, we don't know \n",
    "whether we should interpret all these 0s as \"no investment gain/loss or \"\n",
    "investment gain/loss is unknown.\" Lack of context is always a flag for concern, \n",
    "and one that could trigger fairness-related issues later on. For now, we are \n",
    "going to omit these features from the model, but you are more than welcome to \n",
    "experiment with them if you come up with an idea on how capital gains and \n",
    "losses should be handled.\n",
    "\n",
    "### age\n",
    "\n",
    "\"age\" has a lot of variance, so it might benefit from bucketing to learn\n",
    "fine-grained correlations between income and age, as well as to prevent\n",
    "overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3OT-YVpftEI"
   },
   "source": [
    "## Predicting income\n",
    "\n",
    "Now that we have a better sense of the Adult dataset, we can now begin with creating a neural network to predict income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bt-rQvJLx4Hm"
   },
   "outputs": [],
   "source": [
    "def extract_label(data):\n",
    "    # Drop empty rows.\n",
    "    data = data.dropna(how=\"any\", axis=0)\n",
    "\n",
    "    # Separate DataFrame into two Numpy arrays\"\n",
    "    labels = np.where(data[\"income_bracket\"] == \">50K\", 1.0, 0.0)\n",
    "    features = data.drop(\"income_bracket\", axis=1)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "dat_x, dat_y = extract_label(train_df)\n",
    "test_x, test_y = extract_label(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mz2sts6IjBO"
   },
   "source": [
    "### Represent Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WqAbug6jePb"
   },
   "source": [
    "#### Make Age a Categorical Feature\n",
    "\n",
    "If you chose `age` when completing **FairAware Task #3**, you will have noticed that we suggested *bucketing* (also known as *binning*) this feature, grouping together similar ages into different groups. This might help the model generalize better across age. As such, we will convert `age` from a numeric feature (technically, an [ordinal feature](https://en.wikipedia.org/wiki/Ordinal_data)) to a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxVm8X15yLR7"
   },
   "outputs": [],
   "source": [
    "age_buckets = [x for x in range(10, 100, 10)]\n",
    "dat_x[\"age_bucket\"] = pd.cut(dat_x[\"age\"], bins=age_buckets).astype(str)\n",
    "test_x[\"age_bucket\"] = pd.cut(test_x[\"age\"], bins=age_buckets).astype(str)\n",
    "\n",
    "# numerical_features = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "# categorical_features = [\"age_bucket\", \"occupation\", \"native_country\", \"gender\", \"race\", \"education\", \"marital_status\", \"relationship\", \"workclass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lx4JuLdi7jw"
   },
   "source": [
    "#### Consider Key Subgroups\n",
    "\n",
    "When performing feature engineering, it's important to keep in mind that you may be working with data drawn from individuals belonging to subgroups, for which you'll want to evaluate model performance separately.\n",
    "\n",
    "**_NOTE:_** *In this context, a subgroup is defined as a group of individuals who share a given characteristic—such as race, gender, or sexual orientation—that merits special consideration when evaluating a model with fairness in mind.*\n",
    "\n",
    "When we want our models to mitigate, or leverage, the learned signal of a characteristic pertaining to a subgroup, we will want to use different kinds of tools and techniques—**most of which are still actively being researched and developed**. You can find a list of related research work and techniques at our [Responsible AI Practices](https://ai.google/responsibilities/responsible-ai-practices/?category=fairness) page.\n",
    "\n",
    "As you work with different variables and define tasks for them, it can be useful to think about what comes next. For example, *where are the places where the interaction of the variable and the task could be a concern?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5aD1OM8egad9"
   },
   "source": [
    "### Define the Model Features\n",
    "\n",
    "Now we can explicitly define which feature we will include in our model.\n",
    "\n",
    "We'll consider `gender` a subgroup and save it in a separate `subgroup_variables` list, so we can add special handling for it as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nYSMg67jWaA"
   },
   "source": [
    "### Train a Deep Neural Net Model on Adult Dataset\n",
    "\n",
    "With the features now ready to go, we can try predicting income using deep learning.\n",
    "\n",
    "For the sake of simplicity, we are going to keep the neural network architecture light by simply **defining a feed-forward neural network with two hidden layers**.\n",
    "\n",
    "But first, we have to convert our high-dimensional categorical features into a low-dimensional and dense real-valued vector, which we call an embedding vector. Luckily, ```indicator_column``` (think of it as one-hot encoding) and ```embedding_column``` (that converts sparse features into dense features) helps us streamline the process.\n",
    "\n",
    "Based on our analysis of the data set from previous FairAware Tasks, we are going to move forward with the following features:\n",
    "\n",
    "- One-hot encode\n",
    "    -   `workclass`\n",
    "    -   `education`\n",
    "    -   `age_bucket`\n",
    "    -   `relationship`\n",
    "\n",
    "- Embedding\n",
    "    -   `native_country`\n",
    "    -   `occupation`\n",
    "\n",
    "All other features will be omitted from training — but you are welcome to experiment. `gender` is the only feature that will be used to filter the test set for subgroup evaluation purposes.\n",
    "\n",
    "The following cell creates the deep columns required to define the input layer of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_features = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"age_bucket\",\n",
    "    \"relationship\",\n",
    "]\n",
    "embedding_layers = [\n",
    "    \"native_country\",\n",
    "    \"occupation\",\n",
    "]\n",
    "selected_features = ohe_features + embedding_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic model (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pd.get_dummies(dat_x[selected_features]), dat_y\n",
    "X_test, y_test = pd.get_dummies(test_x[selected_features]), test_y\n",
    "\n",
    "for col in X.columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "X_test = X_test[X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n",
      "Training set: accuracy - 0.8326 | AUC - 0.7403\n",
      "    Test set: accuracy - 0.8296 | AUC - 0.7347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leyan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/leyan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(random_state=42, max_iter=500).fit(X, y)\n",
    "train_pred, test_pred = logistic_model.predict(X), logistic_model.predict(X_test)\n",
    "\n",
    "train_pred = torch.Tensor(train_pred)\n",
    "y = torch.Tensor(y).type(torch.LongTensor)\n",
    "\n",
    "test_pred = torch.Tensor(test_pred)\n",
    "y_test = torch.Tensor(y_test).type(torch.LongTensor)\n",
    "\n",
    "print(train_pred.dtype)\n",
    "print(y.dtype)\n",
    "\n",
    "print(\n",
    "    f\"Training set: accuracy - {logistic_model.score(X, y):.4f} | AUC - {AUROC()(train_pred, y).item():.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"    Test set: accuracy - {logistic_model.score(X_test, y_test):.4f} | AUC - {AUROC()(test_pred, y_test).item():.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "for feat in selected_features:\n",
    "    enc = LabelEncoder()\n",
    "    dat_x[feat] = enc.fit_transform(dat_x[feat].to_numpy())\n",
    "    test_x[feat] = enc.transform(test_x[feat].to_numpy())\n",
    "    encoders[feat] = {\"encoder\": enc, \"classes\": enc.classes_.size}\n",
    "\n",
    "dat_x = dat_x[selected_features]\n",
    "test_x = test_x[selected_features]\n",
    "\n",
    "embedding_sizes = [encoders[x][\"classes\"] for x in embedding_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncomeDataset(Dataset):\n",
    "    def __init__(self, x1, x2, y, encoders):\n",
    "        \"\"\"\n",
    "        x1: list of one-hot encoded features\n",
    "        x2: list of embedding features\n",
    "        y: labels\n",
    "        \"\"\"\n",
    "        self.y = torch.tensor(y.reshape(-1, 1), dtype=torch.float)\n",
    "\n",
    "        self.x1 = torch.cat(\n",
    "            [\n",
    "                F.one_hot(\n",
    "                    torch.tensor(feat.to_numpy(), dtype=torch.long),\n",
    "                    num_classes=encoders[col][\"classes\"],\n",
    "                ).float()\n",
    "                for col, feat in x1.iteritems()\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.x2 = torch.tensor(x2.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x1[idx], self.x2[idx], self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "dat = IncomeDataset(dat_x[ohe_features], dat_x[embedding_layers], dat_y, encoders)\n",
    "dat_test = IncomeDataset(\n",
    "    test_x[ohe_features], test_x[embedding_layers], test_y, encoders\n",
    ")\n",
    "\n",
    "validation_set_size = int(len(dat.y) * 0.05)\n",
    "training_set_size = len(dat.y) - validation_set_size\n",
    "dat_train, dat_val = random_split(dat, [training_set_size, validation_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "# pl.seed_everything(42)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "hparams = {\n",
    "    \"layer1_units\": 128,\n",
    "    \"layer2_units\": 64,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 500,\n",
    "    \"L1_regularization_strength\": 0.001,\n",
    "    \"L2_regularization_strength\": 0.001,\n",
    "    \"pred_cutoff\": 0.2,\n",
    "}\n",
    "\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dat_train.dataset.x1.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNModel(\n",
       "  (l_embeddings): ModuleList(\n",
       "    (0): Embedding(1000, 8)\n",
       "    (1): Embedding(1000, 8)\n",
       "  )\n",
       "  (l1): Linear(in_features=53, out_features=128, bias=True)\n",
       "  (l2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (l3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, hparams, embedding_sizes):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.dat_train = dat_train\n",
    "        self.dat_val = dat_val\n",
    "        self.dat_test = dat_test\n",
    "\n",
    "        self.l_embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(num_embeddings=1000, embedding_dim=8)\n",
    "                for _ in embedding_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.l1 = nn.Linear(\n",
    "            37 + len(embedding_sizes) * 8,\n",
    "            hparams[\"layer1_units\"],\n",
    "        )\n",
    "        self.l2 = nn.Linear(hparams[\"layer1_units\"], hparams[\"layer2_units\"])\n",
    "        self.l3 = nn.Linear(hparams[\"layer2_units\"], 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Train embedding layers\n",
    "        x_embedding = [\n",
    "            l(x2[:, i].view(-1, 1)).view(x2.size(0), -1)\n",
    "            for i, l in enumerate(self.l_embeddings)\n",
    "        ]\n",
    "\n",
    "        # Flatten embedding layers\n",
    "        x = torch.cat(x_embedding, dim=1)\n",
    "        x = torch.cat((x1, x), dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "NNModel(hparams, embedding_sizes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "criterion_loss(\n",
       "  (bce_with_logits): BCEWithLogitsLoss()\n",
       "  (L1): L1Loss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class criterion_loss(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(criterion_loss, self).__init__()        \n",
    "        self.L1_regularization_strength = hparams[\"L1_regularization_strength\"]\n",
    "        self.bce_with_logits = nn.BCEWithLogitsLoss()\n",
    "        self.L1 = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, y_hat, y_pred, y):\n",
    "        return self.bce_with_logits(\n",
    "            y_hat, y\n",
    "        ) + self.L1_regularization_strength * self.L1(y_pred, y)\n",
    "    \n",
    "    \n",
    "criterion = criterion_loss(hparams)           \n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leyan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.1s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 001/020 ]           train_loss = 0.58040 train_accuracy = 0.68534 train_AUC = 0.74429  |           valid_loss = 0.34621 val_accuracy = 0.71050 val_AUC = 0.79881'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 002/020 ]           train_loss = 0.37267 train_accuracy = 0.74192 train_AUC = 0.78754  |           valid_loss = 0.34274 val_accuracy = 0.73500 val_AUC = 0.81019'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 003/020 ]           train_loss = 0.36692 train_accuracy = 0.75115 train_AUC = 0.79316  |           valid_loss = 0.33127 val_accuracy = 0.79225 val_AUC = 0.83963'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 004/020 ]           train_loss = 0.36678 train_accuracy = 0.75482 train_AUC = 0.79375  |           valid_loss = 0.32742 val_accuracy = 0.76000 val_AUC = 0.81758'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 005/020 ]           train_loss = 0.36590 train_accuracy = 0.75314 train_AUC = 0.79244  |           valid_loss = 0.33619 val_accuracy = 0.73050 val_AUC = 0.81102'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 006/020 ]           train_loss = 0.36260 train_accuracy = 0.75494 train_AUC = 0.79653  |           valid_loss = 0.32633 val_accuracy = 0.76400 val_AUC = 0.82299'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 007/020 ]           train_loss = 0.36241 train_accuracy = 0.75747 train_AUC = 0.79745  |           valid_loss = 0.37213 val_accuracy = 0.67800 val_AUC = 0.78509'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 008/020 ]           train_loss = 0.36214 train_accuracy = 0.75536 train_AUC = 0.79644  |           valid_loss = 0.32333 val_accuracy = 0.77450 val_AUC = 0.82170'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 009/020 ]           train_loss = 0.36153 train_accuracy = 0.75891 train_AUC = 0.79788  |           valid_loss = 0.33435 val_accuracy = 0.73400 val_AUC = 0.81259'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 010/020 ]           train_loss = 0.36037 train_accuracy = 0.75835 train_AUC = 0.79822  |           valid_loss = 0.32127 val_accuracy = 0.77500 val_AUC = 0.81992'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 011/020 ]           train_loss = 0.35932 train_accuracy = 0.75808 train_AUC = 0.79775  |           valid_loss = 0.32227 val_accuracy = 0.78550 val_AUC = 0.82227'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 012/020 ]           train_loss = 0.36038 train_accuracy = 0.75975 train_AUC = 0.79805  |           valid_loss = 0.33663 val_accuracy = 0.73400 val_AUC = 0.81401'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 013/020 ]           train_loss = 0.36097 train_accuracy = 0.75787 train_AUC = 0.79727  |           valid_loss = 0.32375 val_accuracy = 0.76550 val_AUC = 0.82228'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 014/020 ]           train_loss = 0.35966 train_accuracy = 0.75832 train_AUC = 0.79717  |           valid_loss = 0.32036 val_accuracy = 0.76550 val_AUC = 0.82556'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 015/020 ]           train_loss = 0.35877 train_accuracy = 0.75857 train_AUC = 0.79985  |           valid_loss = 0.32287 val_accuracy = 0.76000 val_AUC = 0.82104'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 016/020 ]           train_loss = 0.35866 train_accuracy = 0.75991 train_AUC = 0.79898  |           valid_loss = 0.31718 val_accuracy = 0.77450 val_AUC = 0.82600'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 017/020 ]           train_loss = 0.35833 train_accuracy = 0.75976 train_AUC = 0.79961  |           valid_loss = 0.31931 val_accuracy = 0.76800 val_AUC = 0.82485'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 018/020 ]           train_loss = 0.35872 train_accuracy = 0.76079 train_AUC = 0.79908  |           valid_loss = 0.33010 val_accuracy = 0.74700 val_AUC = 0.81703'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 019/020 ]           train_loss = 0.35788 train_accuracy = 0.76080 train_AUC = 0.80040  |           valid_loss = 0.32712 val_accuracy = 0.75700 val_AUC = 0.82424'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 020/020 ]           train_loss = 0.35855 train_accuracy = 0.75857 train_AUC = 0.79773  |           valid_loss = 0.32580 val_accuracy = 0.75200 val_AUC = 0.81887'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the build_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "# For now we use the entire dataset for training (no validation set)\n",
    "torch.manual_seed(42)\n",
    "model = None\n",
    "model = NNModel(hparams, embedding_sizes).to(device)\n",
    "criterion = criterion_loss(hparams)   \n",
    "\n",
    "optimizer = torch.optim.Adagrad(\n",
    "            model.parameters(),\n",
    "            lr=hparams[\"learning_rate\"],\n",
    "            weight_decay=hparams[\"L2_regularization_strength\"],\n",
    "        )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "            dat_train,\n",
    "            batch_size=hparams[\"batch_size\"],\n",
    "            num_workers=8,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "            dat_val, batch_size=hparams[\"batch_size\"], num_workers=4\n",
    "        )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "            dat_test, batch_size=hparams[\"batch_size\"], num_workers=4\n",
    "        )\n",
    "\n",
    "losses = []\n",
    "\n",
    "epoch_train_loss = []\n",
    "\n",
    "epoch_valid_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accuracy = []    \n",
    "    train_AUC = []\n",
    "    \n",
    "    for batch in Bar(train_loader):\n",
    "        x1, x2, y = batch\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)       \n",
    "                \n",
    "        y_hat = model(x1, x2)\n",
    "        \n",
    "        y_pred = (y_hat.sigmoid() > hparams[\"pred_cutoff\"]).float()    \n",
    "        \n",
    "        loss = criterion(y_hat, y_pred, y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor).cpu()\n",
    "        y_pred = y_pred.cpu()        \n",
    "       \n",
    "        accuarcy = Accuracy()(y_pred, y)\n",
    "        auc = AUROC()(y_pred, y)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accuracy.append(accuarcy.item())\n",
    "        train_AUC.append(auc.item())\n",
    "        \n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_accuracy = sum(train_accuracy) / len(train_accuracy)\n",
    "    train_AUC = sum(train_AUC) / len(train_AUC)\n",
    "    \n",
    "    epoch_train_loss.append(train_loss)\n",
    "   \n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    model.eval()\n",
    "    \n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []    \n",
    "    valid_AUC = []\n",
    "    \n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in Bar(valid_loader):\n",
    "        x1, x2, y = batch\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)       \n",
    "                \n",
    "        y_hat = model(x1, x2)\n",
    "        \n",
    "        y_pred = (y_hat.sigmoid() > hparams[\"pred_cutoff\"]).float()    \n",
    "        \n",
    "        loss = criterion(y_hat, y_pred, y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor).cpu()\n",
    "        y_pred = y_pred.cpu()        \n",
    "       \n",
    "        accuarcy = Accuracy()(y_pred, y)\n",
    "        auc = AUROC()(y_pred, y)      \n",
    "        \n",
    "        # Record the loss and accuracy.\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accuracy.append(accuarcy.item())\n",
    "        valid_AUC.append(auc.item())\n",
    "        \n",
    "     # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_accuracy = sum(valid_accuracy) / len(valid_accuracy)\n",
    "    valid_AUC = sum(valid_AUC) / len(valid_AUC)\n",
    "    \n",
    "    epoch_valid_loss.append(valid_loss)\n",
    "        \n",
    "    # Print the information.\n",
    "    display(f\"[ Epoch | {epoch + 1:03d}/{epochs:03d} ] \\\n",
    "          train_loss = {train_loss:.5f} train_accuracy = {train_accuracy:.5f} train_AUC = {train_AUC:.5f}  | \\\n",
    "          valid_loss = {valid_loss:.5f} val_accuracy = {valid_accuracy:.5f} val_AUC = {valid_AUC:.5f}\")\n",
    "\n",
    "print(\"Defined the build_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15060/15060: [==============================>.] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_loss': 0.35960320599617496,\n",
       " 'avg_test_accuracy': 0.7541935443878174,\n",
       " 'avg_test_precision': 0.5002116541708669,\n",
       " 'avg_test_recall': 0.8815959845819781,\n",
       " 'avg_test_AUC': 0.7970910976009984}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Validation ----------\n",
    "# Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "model.eval()\n",
    "\n",
    "# These are used to record information in validation.\n",
    "test_loss = []\n",
    "test_accuracy = []  \n",
    "test_precision= [] \n",
    "test_recall= [] \n",
    "test_AUC = []\n",
    "\n",
    "# Iterate the validation set by batches.\n",
    "for batch in Bar(test_loader):\n",
    "    x1, x2, y = batch\n",
    "    x1, x2, y = x1.to(device), x2.to(device), y.to(device)       \n",
    "\n",
    "    y_hat = model(x1, x2)\n",
    "\n",
    "    y_pred = (y_hat.sigmoid() > hparams[\"pred_cutoff\"]).float()    \n",
    "\n",
    "    loss = criterion(y_hat, y_pred, y)\n",
    "\n",
    "    y = y.type(torch.LongTensor).cpu()\n",
    "    y_pred = y_pred.cpu()        \n",
    "\n",
    "    accuarcy = Accuracy()(y_pred, y)\n",
    "    precision = Precision()(y_pred, y)\n",
    "    recall = Recall()(y_pred, y)\n",
    "    \n",
    "    auc = AUROC()(y_pred, y)      \n",
    "\n",
    "    # Record the loss and accuracy.\n",
    "    test_loss.append(loss.item())\n",
    "    test_accuracy.append(accuarcy.item())\n",
    "    test_precision.append(precision.item())\n",
    "    test_recall.append(recall.item())\n",
    "    test_AUC.append(auc.item())\n",
    "\n",
    " # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "avg_loss = sum(test_loss) / len(test_loss)\n",
    "avg_acc = sum(test_accuracy) / len(test_accuracy)\n",
    "avg_precision = sum(test_precision) / len(test_precision)\n",
    "avg_recall = sum(test_recall) / len(test_recall)\n",
    "avg_AUC = sum(test_AUC) / len(test_AUC)\n",
    "\n",
    "{\n",
    "    \"avg_test_loss\": avg_loss,\n",
    "    \"avg_test_accuracy\": avg_acc,\n",
    "    \"avg_test_precision\": avg_precision,\n",
    "    \"avg_test_recall\": avg_recall,\n",
    "    \"avg_test_AUC\": avg_AUC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "hyperparams = {\n",
    "    \"layer1_units\": 128,\n",
    "    \"layer2_units\": 64,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 500,\n",
    "    \"L1_regularization_strength\": 0.001,\n",
    "    \"L2_regularization_strength\": 0.001,\n",
    "    \"pred_cutoff\": 0.2,\n",
    "}\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "            TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "            TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "            FN += 1\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 001/020 ]           train_loss = 0.58040 train_accuracy = 0.68534 train_AUC = 0.74429  |           valid_loss = 0.34621 val_accuracy = 0.71050 val_AUC = 0.79881'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 002/020 ]           train_loss = 0.37267 train_accuracy = 0.74192 train_AUC = 0.78754  |           valid_loss = 0.34274 val_accuracy = 0.73500 val_AUC = 0.81019'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 003/020 ]           train_loss = 0.36692 train_accuracy = 0.75115 train_AUC = 0.79316  |           valid_loss = 0.33127 val_accuracy = 0.79225 val_AUC = 0.83963'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 004/020 ]           train_loss = 0.36678 train_accuracy = 0.75482 train_AUC = 0.79375  |           valid_loss = 0.32742 val_accuracy = 0.76000 val_AUC = 0.81758'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 005/020 ]           train_loss = 0.36590 train_accuracy = 0.75314 train_AUC = 0.79244  |           valid_loss = 0.33619 val_accuracy = 0.73050 val_AUC = 0.81102'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 006/020 ]           train_loss = 0.36260 train_accuracy = 0.75494 train_AUC = 0.79653  |           valid_loss = 0.32633 val_accuracy = 0.76400 val_AUC = 0.82299'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 007/020 ]           train_loss = 0.36241 train_accuracy = 0.75747 train_AUC = 0.79745  |           valid_loss = 0.37213 val_accuracy = 0.67800 val_AUC = 0.78509'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 008/020 ]           train_loss = 0.36214 train_accuracy = 0.75536 train_AUC = 0.79644  |           valid_loss = 0.32333 val_accuracy = 0.77450 val_AUC = 0.82170'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 009/020 ]           train_loss = 0.36153 train_accuracy = 0.75891 train_AUC = 0.79788  |           valid_loss = 0.33435 val_accuracy = 0.73400 val_AUC = 0.81259'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 010/020 ]           train_loss = 0.36037 train_accuracy = 0.75835 train_AUC = 0.79822  |           valid_loss = 0.32127 val_accuracy = 0.77500 val_AUC = 0.81992'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 011/020 ]           train_loss = 0.35932 train_accuracy = 0.75808 train_AUC = 0.79775  |           valid_loss = 0.32227 val_accuracy = 0.78550 val_AUC = 0.82227'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 012/020 ]           train_loss = 0.36038 train_accuracy = 0.75975 train_AUC = 0.79805  |           valid_loss = 0.33663 val_accuracy = 0.73400 val_AUC = 0.81401'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 013/020 ]           train_loss = 0.36097 train_accuracy = 0.75787 train_AUC = 0.79727  |           valid_loss = 0.32375 val_accuracy = 0.76550 val_AUC = 0.82228'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 014/020 ]           train_loss = 0.35966 train_accuracy = 0.75832 train_AUC = 0.79717  |           valid_loss = 0.32036 val_accuracy = 0.76550 val_AUC = 0.82556'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 015/020 ]           train_loss = 0.35877 train_accuracy = 0.75857 train_AUC = 0.79985  |           valid_loss = 0.32287 val_accuracy = 0.76000 val_AUC = 0.82104'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 016/020 ]           train_loss = 0.35866 train_accuracy = 0.75991 train_AUC = 0.79898  |           valid_loss = 0.31718 val_accuracy = 0.77450 val_AUC = 0.82600'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 017/020 ]           train_loss = 0.35833 train_accuracy = 0.75976 train_AUC = 0.79961  |           valid_loss = 0.31931 val_accuracy = 0.76800 val_AUC = 0.82485'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 018/020 ]           train_loss = 0.35872 train_accuracy = 0.76079 train_AUC = 0.79908  |           valid_loss = 0.33010 val_accuracy = 0.74700 val_AUC = 0.81703'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 019/020 ]           train_loss = 0.35788 train_accuracy = 0.76080 train_AUC = 0.80040  |           valid_loss = 0.32712 val_accuracy = 0.75700 val_AUC = 0.82424'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28654/28654: [===============================>] - ETA 0.0s\n",
      "1508/1508: [========================>.......] - ETA 0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[ Epoch | 020/020 ]           train_loss = 0.35855 train_accuracy = 0.75857 train_AUC = 0.79773  |           valid_loss = 0.32580 val_accuracy = 0.75200 val_AUC = 0.81887'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the build_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "# For now we use the entire dataset for training (no validation set)\n",
    "torch.manual_seed(42)\n",
    "model = None\n",
    "model = NNModel(hparams, embedding_sizes).to(device)\n",
    "criterion = criterion_loss(hparams)   \n",
    "\n",
    "optimizer = torch.optim.Adagrad(\n",
    "            model.parameters(),\n",
    "            lr=hparams[\"learning_rate\"],\n",
    "            weight_decay=hparams[\"L2_regularization_strength\"],\n",
    "        )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "            dat_train,\n",
    "            batch_size=hparams[\"batch_size\"],\n",
    "            num_workers=8,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "            dat_val, batch_size=hparams[\"batch_size\"], num_workers=4\n",
    "        )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "            dat_test, batch_size=hparams[\"batch_size\"], num_workers=4\n",
    "        )\n",
    "\n",
    "losses = []\n",
    "\n",
    "epoch_train_loss = []\n",
    "\n",
    "epoch_valid_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accuracy = []    \n",
    "    train_AUC = []\n",
    "    \n",
    "    for batch in Bar(train_loader):\n",
    "        x1, x2, y = batch\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)       \n",
    "                \n",
    "        y_hat = model(x1, x2)\n",
    "        \n",
    "        y_pred = (y_hat.sigmoid() > hparams[\"pred_cutoff\"]).float()    \n",
    "        \n",
    "        loss = criterion(y_hat, y_pred, y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor).cpu()\n",
    "        y_pred = y_pred.cpu()        \n",
    "       \n",
    "        accuarcy = Accuracy()(y_pred, y)\n",
    "        auc = AUROC()(y_pred, y)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accuracy.append(accuarcy.item())\n",
    "        train_AUC.append(auc.item())\n",
    "        \n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_accuracy = sum(train_accuracy) / len(train_accuracy)\n",
    "    train_AUC = sum(train_AUC) / len(train_AUC)\n",
    "    \n",
    "    epoch_train_loss.append(train_loss)\n",
    "   \n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    model.eval()\n",
    "    \n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []    \n",
    "    valid_AUC = []\n",
    "    \n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in Bar(valid_loader):\n",
    "        x1, x2, y = batch\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)       \n",
    "                \n",
    "        y_hat = model(x1, x2)\n",
    "        \n",
    "        y_pred = (y_hat.sigmoid() > hparams[\"pred_cutoff\"]).float()    \n",
    "        \n",
    "        loss = criterion(y_hat, y_pred, y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor).cpu()\n",
    "        y_pred = y_pred.cpu()        \n",
    "       \n",
    "        accuarcy = Accuracy()(y_pred, y)\n",
    "        auc = AUROC()(y_pred, y)      \n",
    "        \n",
    "        # Record the loss and accuracy.\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accuracy.append(accuarcy.item())\n",
    "        valid_AUC.append(auc.item())\n",
    "        \n",
    "     # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_accuracy = sum(valid_accuracy) / len(valid_accuracy)\n",
    "    valid_AUC = sum(valid_AUC) / len(valid_AUC)\n",
    "    \n",
    "    epoch_valid_loss.append(valid_loss)\n",
    "        \n",
    "    # Print the information.\n",
    "    display(f\"[ Epoch | {epoch + 1:03d}/{epochs:03d} ] \\\n",
    "          train_loss = {train_loss:.5f} train_accuracy = {train_accuracy:.5f} train_AUC = {train_AUC:.5f}  | \\\n",
    "          valid_loss = {valid_loss:.5f} val_accuracy = {valid_accuracy:.5f} val_AUC = {valid_AUC:.5f}\")\n",
    "\n",
    "print(\"Defined the build_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15060/15060: [==============================>.] - ETA 0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_loss': 0.35960320599617496,\n",
       " 'avg_test_accuracy': 0.7541935443878174,\n",
       " 'avg_test_precision': 0.5002116541708669,\n",
       " 'avg_test_recall': 0.8815959845819781,\n",
       " 'avg_test_AUC': 0.7970910976009984}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Validation ----------\n",
    "# Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "model.eval()\n",
    "\n",
    "# These are used to record information in validation.\n",
    "test_loss = []\n",
    "test_accuracy = []  \n",
    "test_precision= [] \n",
    "test_recall= [] \n",
    "test_AUC = []\n",
    "\n",
    "\n",
    "TPs, FPs, TNs, FNs = 0, 0, 0, 0\n",
    "\n",
    "# Iterate the validation set by batches.\n",
    "for batch in Bar(test_loader):\n",
    "    x1, x2, y = batch\n",
    "    x1, x2, y = x1.to(device), x2.to(device), y.to(device)       \n",
    "\n",
    "    y_hat = model(x1, x2)\n",
    "\n",
    "    y_pred = (y_hat.sigmoid() > hparams[\"pred_cutoff\"]).float()    \n",
    "\n",
    "    loss = criterion(y_hat, y_pred, y)\n",
    "\n",
    "    y = y.type(torch.LongTensor).cpu()\n",
    "    y_pred = y_pred.cpu()   \n",
    "    \n",
    "    TP, FP, TN, FN = perf_measure(y, y_pred)\n",
    "    \n",
    "    TPs = TPs + TP\n",
    "    FPs = FPs + FP\n",
    "    TNs = TNs + TN\n",
    "    FNs = FNs + FN\n",
    "\n",
    "    accuarcy = Accuracy()(y_pred, y)\n",
    "    precision = Precision()(y_pred, y)\n",
    "    recall = Recall()(y_pred, y)\n",
    "    \n",
    "    auc = AUROC()(y_pred, y)      \n",
    "\n",
    "    # Record the loss and accuracy.\n",
    "    test_loss.append(loss.item())\n",
    "    test_accuracy.append(accuarcy.item())\n",
    "    test_precision.append(precision.item())\n",
    "    test_recall.append(recall.item())\n",
    "    test_AUC.append(auc.item())\n",
    "\n",
    " # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "avg_loss = sum(test_loss) / len(test_loss)\n",
    "avg_acc = sum(test_accuracy) / len(test_accuracy)\n",
    "avg_precision = sum(test_precision) / len(test_precision)\n",
    "avg_recall = sum(test_recall) / len(test_recall)\n",
    "avg_AUC = sum(test_AUC) / len(test_AUC)\n",
    "\n",
    "{\n",
    "    \"avg_test_loss\": avg_loss,\n",
    "    \"avg_test_accuracy\": avg_acc,\n",
    "    \"avg_test_precision\": avg_precision,\n",
    "    \"avg_test_recall\": avg_recall,\n",
    "    \"avg_test_AUC\": avg_AUC\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7j0LrXMGlTDl"
   },
   "source": [
    "You can try retraining the model using different parameters. If you leave the parameters as is, then you see that this relatively simple deep neural net does a decent job in predicting income with an **overall accuracy of 0.8317** and an **AUC of 0.8817**. \n",
    "\n",
    "**But evaluation metrics with respect to subgroups are missing.** We will cover some of the ways you can evaluate at the subgroup level in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbwmbnUUU1kY"
   },
   "source": [
    "## Evaluating for Fairness Using a Confusion Matrix\n",
    "\n",
    "While evaluating the overall performance of the model gives us some insight into its quality, it doesn't give us much insight into how well our model performs for different subgroups.  \n",
    "\n",
    "When evaluating a model for fairness, it's important to determine whether prediction errors are uniform across subgroups or whether certain subgroups are more susceptible to certain prediction errors than others. \n",
    "\n",
    "A key tool for comparing the prevalence of different types of model errors is a *confusion matrix*. Recall from the [Classification module of Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative) that a confusion matrix is a grid that plots predictions vs. ground truth for your model, and tabulates statistics summarizing how often your model made the correct prediction and how often it made the wrong prediction. \n",
    "\n",
    "Let's start by creating a binary confusion matrix for our income-prediction model—binary because our label (`income_bracket`) has only two possible values (`<50K` or `>50K`). We'll define an income of `>50K` as our **positive label**, and an income of `<50k` as our **negative label**.\n",
    "\n",
    "**NOTE:** *Positive* and *negative* in this context should not be interpreted as value judgments (we are not suggesting that someone who earns more than 50k a year is a better person than someone who earns less than 50k). They are just standard terms used to distinguish between the two possible predictions the model can make.\n",
    "\n",
    "Cases where the model makes the correct prediction (the prediction matches the ground truth) are classified as **true**, and cases where the model makes the wrong prediction are classified as **false**.\n",
    "\n",
    "Our confusion matrix thus represents four possible states:\n",
    "\n",
    "* **true positive**: Model predicts `>50K`, and that is the ground truth.\n",
    "* **true negative**: Model predicts `<50K`, and that is the ground truth.\n",
    "* **false positive**: Model predicts `>50K`, and that contradicts reality.\n",
    "* **false negative**: Model predicts `<50K`, and that contradicts reality.\n",
    "\n",
    "**NOTE:** If desired, we can use the number of outcomes in each of these states to calculate secondary evaluation metrics, such as [precision and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsUj_XZHU_mI"
   },
   "source": [
    "### Plot the Confusion Matrix\n",
    "\n",
    "Since we've already defined which metrics we're interested in back when we defined and compiled our model, all we have to do now is:\n",
    "\n",
    "\n",
    "1.   Define a function that will help us visualize the heatmap.\n",
    "2.   Select which subgroup we're interested in, then pass that subgroup selection into `tf.keras.Model.predict()` for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ouE72GWSxu1j"
   },
   "outputs": [],
   "source": [
    "# @title Define Function to Visualize Binary Confusion Matrix\n",
    "def plot_confusion_matrix(confusion_matrix, class_names, subgroup, figsize=(8, 6)):\n",
    "    # We're taking our calculated binary confusion matrix that's already in the\n",
    "    # form of an array and turning it into a pandas DataFrame because it's a lot\n",
    "    # easier to work with a pandas DataFrame when visualizing a heat map in\n",
    "    # Seaborn.\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names,)\n",
    "\n",
    "    rcParams.update(\n",
    "        {\"font.family\": \"sans-serif\", \"font.sans-serif\": [\"Liberation Sans\"],}\n",
    "    )\n",
    "\n",
    "    sns.set_context(\"notebook\", font_scale=1.25)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.title(\"Confusion Matrix for Performance Across \" + subgroup)\n",
    "\n",
    "    # Combine the instance (numercial value) with its description\n",
    "    strings = np.asarray(\n",
    "        [[\"True Positives\", \"False Negatives\"], [\"False Positives\", \"True Negatives\"]]\n",
    "    )\n",
    "    labels = (\n",
    "        np.asarray(\n",
    "            [\n",
    "                \"{0:g}\\n{1}\".format(value, string)\n",
    "                for string, value in zip(strings.flatten(), confusion_matrix.flatten())\n",
    "            ]\n",
    "        )\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    heatmap = sns.heatmap(\n",
    "        df_cm, annot=labels, fmt=\"\", linewidths=2.0, cmap=sns.color_palette(\"GnBu_d\")\n",
    "    )\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\")\n",
    "    heatmap.xaxis.set_ticklabels(\n",
    "        heatmap.xaxis.get_ticklabels(), rotation=45, ha=\"right\"\n",
    "    )\n",
    "    plt.ylabel(\"References\")\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUvBYtwXVzlQ"
   },
   "source": [
    "Now that we have all the necessary functions defined, we can now compute the binary confusion matrix and evaluation metrics using the outcomes from [our deep neural net model](#scrollTo=3nYSMg67jWaA). The output of this cell is a tabbed view, which allows us to toggle between the confusion matrix and evaluation metrics table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9enf_Jfi-AVS"
   },
   "source": [
    "### FairAware Task #4\n",
    "\n",
    "Use the form below to generate confusion matrices for the two gender subgroups: `Female` and `Male`. Compare the number of False Positives and False Negatives for each subgroup. Are there any significant disparities in error rates that suggest the model performs better for one subgroup than another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "5TBzaWs1VKTa",
    "outputId": "7c8744b5-e9e7-4efa-d90c-f3798bc73a4b"
   },
   "outputs": [],
   "source": [
    "# #@title Visualize Binary Confusion Matrix and Compute Evaluation Metrics Per Subgroup\n",
    "CATEGORY  =  \"gender\" #@param {type:\"string\"}\n",
    "SUBGROUP =  \"Male\" #@param {type:\"string\"}\n",
    "\n",
    "# Labels for annotating axes in plot.\n",
    "classes = ['Over $50K', 'Less than $50K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define Function to Visualize Binary Confusion Matrix\n",
    "def plot_confusion_matrix(\n",
    "    confusion_matrix, class_names, subgroup, figsize = (8,6)):\n",
    "    # We're taking our calculated binary confusion matrix that's already in the \n",
    "    # form of an array and turning it into a pandas DataFrame because it's a lot \n",
    "    # easier to work with a pandas DataFrame when visualizing a heat map in \n",
    "    # Seaborn.\n",
    "    df_cm = pd.DataFrame(\n",
    "      confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "\n",
    "    rcParams.update({\n",
    "    'font.family':'sans-serif',\n",
    "    'font.sans-serif':['Liberation Sans'],\n",
    "    })\n",
    "\n",
    "    sns.set_context(\"notebook\", font_scale=1.25)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.title('Confusion Matrix for Performance Across ' + subgroup)\n",
    "\n",
    "    # Combine the instance (numercial value) with its description\n",
    "    strings = np.asarray([['True Positives', 'False Negatives'],\n",
    "                        ['False Positives', 'True Negatives']])\n",
    "    labels = (np.asarray(\n",
    "      [\"{0:g}\\n{1}\".format(value, string) for string, value in zip(\n",
    "          strings.flatten(), confusion_matrix.flatten())])).reshape(2, 2)\n",
    "\n",
    "    heatmap = sns.heatmap(df_cm, annot=labels, fmt=\"\", \n",
    "      linewidths=2.0, cmap=sns.color_palette(\"GnBu_d\"));\n",
    "    heatmap.yaxis.set_ticklabels(\n",
    "      heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    heatmap.xaxis.set_ticklabels(\n",
    "      heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n",
    "    plt.ylabel('References')\n",
    "    plt.xlabel('Predictions')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCURACY</th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.7542</td>\n",
       "      <td>0.8816</td>\n",
       "      <td>0.8816</td>\n",
       "      <td>0.7971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACCURACY  PRECISION  RECALL    AUC\n",
       "Male    0.7542     0.8816  0.8816 0.7971"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# subgroup_results = model.evaluate(x=features, y=labels, verbose=0)\n",
    "# tp - fp - tn - fn \n",
    "confusion_matrix = np.array([[TPs, FNs], \n",
    "                             [FPs, TNs]])\n",
    "\n",
    "subgroup_performance_metrics = {\n",
    "    'ACCURACY': avg_acc,\n",
    "    'PRECISION': avg_recall, \n",
    "    'RECALL': avg_recall,\n",
    "    'AUC': avg_AUC\n",
    "}\n",
    "performance_df = pd.DataFrame(subgroup_performance_metrics, index=[SUBGROUP])\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix, classes, SUBGROUP);\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TF3B5h3c-7Fb"
   },
   "source": [
    "### Solution\n",
    "\n",
    "Click below for some insights we uncovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhKR49AT_5ZK"
   },
   "source": [
    "Using default parameters, you may find that the model performs better for female than male. Specifically, in our run, we found that both accuracy and AUC for female (0.9137 and 0.9089, respectively) outperformed male (0.7923 and 0.8549, respectively). What is going on here?\n",
    "\n",
    "Notice the number of true positives (top-left corner) for female is way lower compared to male (479 to 3822). Recall that in Task #1 we noticed a disproportionately high representation of male in the data set (almost 2-to-1). If you further explore the data set using Facets Dive in Task #2 by setting the color to `income_bracket` and one of the axes to `gender`, then you will also find a disproportionately small number of female examples in the higher income bracket, our positive label. \n",
    "\n",
    "What this is all suggesting is that the model is **overfitting, particuarly with respect to female and lower income bracket**. In other words, this model will not generalize well, particularly with female data, as it does not have enough positive examples for the model to learn from. It is **not doing that much better with male, either, as there is a disproportionately small number of high income bracket compared to low income bracket** — though not nearly as poorly represented as with female.\n",
    "\n",
    "Hopefully going through this confusion matrix demonstration you find that the results varies slightly from the overall performance metrics, highlighting the importance of evaluating model performance across subgroup rather than in aggregate.\n",
    "\n",
    "In your work, make sure that you make a good decision about the tradeoffs between false positives, false negatives, true positives, and true negatives. For example, you may want a very low false positive rate, but a high true positive rate. Or you may want a high precision, but a low recall is okay.  \n",
    "\n",
    "**Choose your evaluation metrics in light of these desired tradeoffs.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "J8daw3YOIAXH",
    "xFxZOg55lWJE",
    "l-K-xqksm-X3",
    "TXkkHYyJ98_k",
    "91wjnZFpPWw-",
    "KlF-lQ8yQ69b",
    "qZ-9vJgSEpHj",
    "7YVH8hYfSjer",
    "2lx4JuLdi7jw",
    "TF3B5h3c-7Fb"
   ],
   "name": "intro_to_fairness.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
